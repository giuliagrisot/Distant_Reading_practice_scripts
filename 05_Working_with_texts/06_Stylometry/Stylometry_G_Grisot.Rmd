---
title: "Stylometry with R"
author: "Giulia Grisot"
date: "2024-03-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# also scientific notation
options(scipen=999)

```

# Stylometry with stylo in R

## Introduction

Stylometry is the quantitative study of literary style through computational analysis. It is a subfield of digital humanities that uses statistical techniques from the field of machine learning to analyze the linguistic features of texts. Stylometry can be used to identify the authorship of texts, to study the evolution of an author's style over time, and to compare the styles of different authors. In this tutorial, we will use the R programming language to perform a stylometric analysis of a corpus of texts.

## Getting Started

First, we want to change our working directory and set it to the folder Stylometry, where a smaller version of our corpus has been created for this exercise.

```{r, eval=FALSE}
# 
# # first let's check where we are
# getwd()
# 
# # let's set up the working directory if it's not already set
# setwd("YOUR PATH TO THE FOLDER/scripts/Stylometry")
# 
# # also, remember you can just navigate to the folder in the RStudio interface and set the working directory from there, cliking on More -> Set As Working Directory

```


Stylo works by default on the files in the "corpus" folder inside the working directory if you are not there, it will ask you to reach that folder.

# Installing and loading the necessary R packages

Before we can begin our analysis, we need to install the necessary R packages. We will be using the `stylo` package, which provides a suite of tools for stylometric analysis. We will also be using the `tm` package, which provides tools for text mining. If you do not already have these packages installed, you can install them by running the following commands in R:

```{r, packages install, message=FALSE, warning=FALSE}

# install.packages("stylo")
# install.packages("tm")
# install.packages("readtext")
# install.packages("wordcloud")
# install.packages("remotes")
# remotes::install_github("jmclawson/stylo2gg")

```

Once the packages are installed, we can load them into our R session:

```{r, packages load, message=FALSE, warning=FALSE}
library(stylo) # https://www.youtube.com/watch?v=pWOCfZnitdc
library(tm)
library(readtext)
library(wordcloud)
library(stylo2gg) # https://jmclawson.net/posts/introducing-stylo2gg/
library(tidyverse)

```

Great, now we are ready to start our analysis!

# Stylo basics

Stylo is a package for R that provides a suite of tools for stylometric analysis. It can be used to compare the styles of different authors, to identify the authorship of texts, and to study the evolution of an author's style over time. The package provides a graphical user interface (GUI) that allows users to perform stylometric analyses without writing any code, but it also provides a set of functions that can be used to perform stylometric analyses programmatically.

The main function in the stylo package is the `stylo` function, which can be used to perform a variety of stylometric analyses. The `stylo` function takes a corpus of texts as input and returns a list of results, which can be used to visualize the results of the analysis and to perform further analyses.

The `stylo` function can be used to perform a variety of stylometric analyses, including:
- Authorship attribution: determining the authorship of a text based on its style
- Authorship verification: determining whether two texts were written by the same author
- Authorship profiling: identifying the stylistic features that are characteristic of a particular author
- Stylometric profiling: identifying the stylistic features that are characteristic of a particular genre or period
- Stylometric comparison: comparing the styles of different authors or texts
and more

Let's try and use our corpus to perform a stylometric analysis with the `stylo` function.
Stylo takes automatically all the .txt files in the "corpus" folder, and uses them as the corpus for the analysis.


```{r}

# If we just run the stylo function, it will use the default settings, which you can check in the stylo repository on GitHub

# Let's try the default Cluster Analysis (CA) on our corpus

stylo() # select CA in the GUI to try this analysis

```
The default analysis is a "Cluster Analysis" (CA), which is a technique that is used to identify clusters of texts that have similar styles. It takes a distance matrix as input and produces a dendrogram that shows the relationships between different authors or texts. The dendrogram can be used to identify clusters of texts that have similar styles, and to visualize the relationships between different authors or texts.

You can also call `stylo` again and try a different analysis, for example a "Multidimensional Scaling" (MDS) analysis. We don't need toe use the GUI, we can just call the function with the parameters we want to use, and specify that we do not want to use the GUI.

```{r, eval=FALSE}
stylo(gui = FALSE,
      analysis.type = "MDS")

```

The Multidimensional Scaling" (MDS) analysis is a technique that is used to visualize the similarity between texts in a corpus. It takes a distance matrix as input and produces a two-dimensional plot that shows the relative positions of the texts in the corpus. The MDS plot can be used to identify clusters of texts that have similar styles, and to visualize the relationships between different authors or texts. 

Another analyou can perform with stylo is the principal component analysis (PCA). This is a technique that is used to identify the principal components of variation in a corpus of texts. It takes a distance matrix as input and produces a plot that shows the relative positions of the texts in the corpus. The PCA plot can be used to identify clusters of texts that have similar styles, and to visualize the relationships between different authors or texts.

```{r}
stylo() # select PCA in the GUI to try this analysis

```

So far we just run and visualised the results of the analysis. We can also save the results in a variable, and then use it to explore the results in more detail.

The `stylo` function returns a list of results, which can be used to visualize the results of the analysis and to perform further analyses. 

Stylo also saves results in a csv file in the working directory, so you can also check the results there. You can also specify the name of the file where the results will be saved, and the name of the folder where the corpus is located.

If you want to play with parameters, you can have a looka at this video by Maciej Eder: https://www.youtube.com/watch?v=uPHPhIo2Drc 

## Corpus creation - alternative method - paired comparison

In general, we can use the `list.files` function to create a list of file paths, and then read the text files into R using the `readLines` function. This method gives us control over the loading process, and allows us to handle non-text files and specify the encoding of the text files. Here is an example of how we can use this method to load our corpus of texts into R.

We can read the text files into R using the readLines function.

This can be useful if we want for instance to compare the style of two authors, or two periods, or two genres, etc.

```{r, eval=FALSE}

# create a list of file paths

# First, select all files in the "corpus" folder
file_list <- list.files("corpus", full.names = T)

# then prepare an empty list for the texts 
my_texts <- list()
# and run a loop on all the files
for(i in 1:length(file_list)){
  # read text (for .txt files)
  if(grepl(pattern = ".txt", x = file_list[i])){ # if the file is a .txt file
    loaded_file <- readLines(file_list[i], warn = F) # read the file
    loaded_file <- paste(loaded_file, collapse = "\n") # collapse the lines into a single string
  }
  # in case it is an xml file, let's delete the markup
  if(grepl(pattern = ".xml", x = file_list[i])){ # if the file is a .xml file
    loaded_file <- scan(file_list[i], what = "char", encoding = "utf-8", sep = "\n", quiet = TRUE) # read the file
    loaded_file <- delete.markup(loaded_file, markup.type = "xml") # delete the markup
  }
  # If we want to run stylo on the texts, we need to "tokenize" them (i.e. split them into single words)
  # There is a function in the "stylo" package that does it:
  my_texts[[i]] <- stylo::txt.to.words.ext(loaded_file, corpus.lang = "English") # tokenize the text
  # then add correct names to the different texts in the list
  # (we can re-use the names saved in the list_files variable, by deleting the "corpus/" at the beginning)
  names(my_texts)[i] <- gsub(pattern = "corpus/|.txt|.xml", replacement = "", x = file_list[i]) # remove the "corpus/" and the file extension
  # print progress
  print(i) # print the number of the file that has been processed
}

```

Now we can explore the list: each element contains one tokenized text:

```{r}
names(my_texts)
head(my_texts[[1]])
head(my_texts[[2]])

```

Now everything is ready to run stylo on the texts that we have saved in the R list.
We can even call stylo by deactivating the GUI, and setting all the features via R code.

```{r}

# Let's run (again) a Cluster Analysis (CA) on our corpus

results_stylo <- stylo(gui = FALSE, # deactivate the GUI
                       corpus.lang="English", # specify the language
                       analysis.type="CA", # specify the type of analysis
                       # mfw.min=2000, # specify the minimum number of words
                       # mfw.max=2000, # specify the maximum number of words
                       distance.measure="dist.wurzburg", # specify the distance measure
                       parsed.corpus = my_texts) # specify the corpus


```

Same as before, the `stylo` function returns a list of results, which can be used to visualize the results of the analysis and to perform further analyses.

# Exploring the results

We can explore the results of the analysis by looking at the different elements of the results_stylo variable. For example, we can look at the distance table, which shows the distances between different texts in the corpus. We can also look at the frequency table, which shows the frequency of different words in the corpus. We can use the `rownames` and `colnames` functions to see the names of the texts and the words in the tables, and we can use the `sort` function to sort the tables by different criteria.

```{r}

# Explore the results of the analysis
results_stylo$distance.table
# rows are the texts, columns the texts
# the values are the distances between the texts

# Note: the "$" simbol is used to see the sub-section in a structured variable
# In this case, the "distance.table" is a sub-section of the "results_stylo" variable

```

This is practically a correlation matrix, where the values are the distances between the texts.
We can also plot it as such, using the `heatmap` function, including the values of the distances.

```{r}

heatmap(results_stylo$distance.table, # the distance table
        scale = "none", # no scaling
        main = "Distance between texts", # title of the plot
        )

```

Or a portion of the distance table:

```{r}
# for example the one of the first text in our selection
results_stylo$distance.table[1,]

```

Which one is the "closest" text?

```{r}
sort(results_stylo$distance.table[1,])

```
The text with the lowest score is the closest one to the first text in our selection.

We can also see a table with the frequency of all words, which will be useful to see which words are the most frequent in the texts.

```{r}

results_stylo$frequencies.0.culling
# rows are the texts, columns the words

```


We can also produce a list of the most frequent words (let's just see the top 30 for now)

```{r}
# we can use the "colnames" function to see the words in the table
colnames(results_stylo$frequencies.0.culling)[1:30]

```

Or what is the position in the table of a specific word, for example "lights"

```{r}
lights_position <- which(colnames(results_stylo$frequencies.0.culling) == "lights")

lights_position

```

And which author uses "lights" more frequently? Again, the result will show us the position of the word in the table, where the first column is the author where the word is most frequent.

```{r}
# which author uses "lights" more frequently?
sort(results_stylo$frequencies.0.culling[,lights_position], decreasing = T)

```

## 2. Zeta Analysis

We can also use the "oppose" function, still in the "stylo" package, that looks for the most distinctive words of a set of texts in comparison with another set of texts. The method it uses is known as "Zeta Analysis". The corpus should be divided in two parts: a "primary set" where we have the texts of interest, and a "secondary set" to be compared with.

Given that the dendogram showed that Bronte is at one extreme, which indicates her style is somewhat relatively 'more' different from the others, we can use her texts as our primary set, and the texts by all the others as our secondary set.

```{r}

# find the texts written by one author (e.g. Bronte)
Chosen_texts <- which(grepl("Bronte", names(my_texts)))
Chosen_texts

```

Now everything is ready to run an "oppose" analysis

```{r}

# We use the "oppose" function, still in the "stylo" package, which looks for the most distinctive words. The method it uses is known as "Zeta Analysis"
# The corpus should be divided in two parts: a "primary set" where we have the texts of interest and a "secondary set" to be compared with.

# Our primary set are the texts by Bronte
primary_set <- my_texts[Chosen_texts]
# Our secondary set are the texts by all the others
secondary_set <- my_texts[-Chosen_texts]

```

Now everything is ready to run an "oppose" analysis

```{r}

oppose(primary.corpus = primary_set, secondary.corpus = secondary_set)

# In the graphical interface, you can leave things as they are and click on "OK"
# In the R code, you can specify the parameters of the analysis, as we did before

```

Lovely! This plot shows us the most distinctive words for the primary set (in comparison with the secondary set). The 'Avoided' indicates the words that are most distinctive for the primary set, and the 'Preferred' indicates the words that are most distinctive for the secondary set.

# Visualization of the results

```{r, error=FALSE, message=FALSE, warning=FALSE}

# Visualization of the results

# our result_stylo variable contains the results of the last analysis. We can visualize them with the "plot" function.
# The "oppose" analysis has generated a table with the most distinctive words (the "z-scores") for the primary set (in comparison with the secondary set).

# we can also visualize the same table with a word cloud

library(wordcloud)

wordcloud(words = colnames(results_stylo$table.with.all.freqs),
          freq = results_stylo$table.with.all.zscores, 
          max.words = 300,
          random.order = F, 
          colors = brewer.pal(8, "Dark2"))

```

